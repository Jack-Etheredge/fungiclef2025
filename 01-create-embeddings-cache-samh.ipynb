{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate fungiclef2025\n",
    "# python -m ipykernel install --user --name=fungiclef2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "formatted"
    ]
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import argparse\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# import faiss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import open_clip\n",
    "\n",
    "from typing import Sequence, Tuple, Any, Dict, List, Optional, Union\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetimestr = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "formatted"
    ]
   },
   "outputs": [],
   "source": [
    "# path to fungitatsic dataset\n",
    "data_path = Path('~/datasets/fungiclef2025/').expanduser().resolve()\n",
    "# data_path = '/kaggle/input/fungi-clef-2025/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FungiTasticBEIT(torch.nn.Module):\n",
    "    def __init__(self, device, image_size=384):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.image_size = image_size\n",
    "        self.huggingface_id = \"hf-hub:BVRA/beit_base_patch16_384.in1k_ft_df24_384\"\n",
    "        self.mean = [0.5, 0.5, 0.5]\n",
    "        self.std = [0.5, 0.5, 0.5]\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load the model and its associated image processor.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = timm.create_model(self.huggingface_id, img_size=384, pretrained=True)\n",
    "        # self.model = CustomModel(model)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(int(self.image_size * 1.14), interpolation=Image.BICUBIC),\n",
    "            transforms.CenterCrop(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std),\n",
    "        ])\n",
    "        # self.preprocess = transforms.Compose([\n",
    "        # transforms.Resize(self.image_size),\n",
    "        # transforms.CenterCrop(self.image_size),\n",
    "        # transforms.ToTensor(),\n",
    "        # transforms.Normalize(self.mean, self.std),\n",
    "        # ])\n",
    "        \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract normalized feature embeddings from a given image.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError('Model not loaded')\n",
    "        \n",
    "        image_tensor = [self.preprocess(img).to(self.device) for img in image]\n",
    "        image_tensor = torch.stack(image_tensor).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # features = self.model(image_tensor)\n",
    "            features = self.model.forward_features(image_tensor)\n",
    "            features = self.model.forward_head(features, pre_logits=True)\n",
    "            \n",
    "        return self.normalize_embedding(features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_embedding(embs):\n",
    "        \"\"\"\n",
    "        Normalize the embedding vectors to have unit length.\n",
    "        \"\"\"\n",
    "        return torch.nn.functional.normalize(embs.float(), dim=1, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2B(torch.nn.Module):\n",
    "    def __init__(self, device, model_size='b', image_size=434):\n",
    "        \"\"\"\n",
    "        Initialize the DINOv2 feature extractor with support for higher resolution images.\n",
    "        \n",
    "        Args:\n",
    "            device: The device to run the model on (e.g., 'cuda' or 'cpu')\n",
    "            model_size: Size of the DINOv2 model ('s', 'b', 'l', or 'g')\n",
    "            image_size: Size to resize images to\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.preprocess = None\n",
    "        self.model_size = model_size\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load the DINOv2 model and its associated image transforms.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Map model size to the appropriate model name\n",
    "        model_name = f\"dinov2_vit{self.model_size}14\"  # s, b, l, g\n",
    "\n",
    "        # https://github.com/facebookresearch/dinov2?tab=readme-ov-file\n",
    "        # The embedding dimension is:\n",
    "        # 384 for ViT-S.\n",
    "        # 768 for ViT-B.\n",
    "        # 1024 for ViT-L.\n",
    "        # 1536 for ViT-g.\n",
    "        \n",
    "        # Load the model from torch hub\n",
    "        self.model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # # Define preprocessing transforms with larger resize dimensions\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(int(self.image_size * 1.14), interpolation=Image.BICUBIC),  # Resize larger, then crop\n",
    "            transforms.CenterCrop(self.image_size),  # Crop to exact size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract normalized feature embeddings from a given image.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError('Model not loaded')\n",
    "        \n",
    "        image_tensor = [self.preprocess(img).to(self.device) for img in image]\n",
    "        image_tensor = torch.stack(image_tensor).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model(image_tensor)\n",
    "            \n",
    "        return self.normalize_embedding(features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_embedding(embs):\n",
    "        \"\"\"\n",
    "        Normalize the embedding vectors to have unit length.\n",
    "        \"\"\"\n",
    "        return torch.nn.functional.normalize(embs.float(), dim=1, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2L(torch.nn.Module):\n",
    "    def __init__(self, device, model_size='l', image_size=518):\n",
    "        \"\"\"\n",
    "        Initialize the DINOv2 feature extractor with support for higher resolution images.\n",
    "        \n",
    "        Args:\n",
    "            device: The device to run the model on (e.g., 'cuda' or 'cpu')\n",
    "            model_size: Size of the DINOv2 model ('s', 'b', 'l', or 'g')\n",
    "            image_size: Size to resize images to\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.preprocess = None\n",
    "        self.model_size = model_size\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load the DINOv2 model and its associated image transforms.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Map model size to the appropriate model name\n",
    "        model_name = f\"dinov2_vit{self.model_size}14\"  # s, b, l, g\n",
    "\n",
    "        # https://github.com/facebookresearch/dinov2?tab=readme-ov-file\n",
    "        # The embedding dimension is:\n",
    "        # 384 for ViT-S.\n",
    "        # 768 for ViT-B.\n",
    "        # 1024 for ViT-L.\n",
    "        # 1536 for ViT-g.\n",
    "        \n",
    "        # Load the model from torch hub\n",
    "        self.model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # # Define preprocessing transforms with larger resize dimensions\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(int(self.image_size * 1.14), interpolation=Image.BICUBIC),  # Resize larger, then crop\n",
    "            transforms.CenterCrop(self.image_size),  # Crop to exact size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract normalized feature embeddings from a given image.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError('Model not loaded')\n",
    "        \n",
    "        image_tensor = [self.preprocess(img).to(self.device) for img in image]\n",
    "        image_tensor = torch.stack(image_tensor).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model(image_tensor)\n",
    "            \n",
    "        return self.normalize_embedding(features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_embedding(embs):\n",
    "        \"\"\"\n",
    "        Normalize the embedding vectors to have unit length.\n",
    "        \"\"\"\n",
    "        return torch.nn.functional.normalize(embs.float(), dim=1, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMViTH(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.mean, self.std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load the model and its associated image processor.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = timm.create_model('samvit_huge_patch16.sa1b', pretrained=True, num_classes=0)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()        \n",
    "        data_config = timm.data.resolve_model_data_config(self.model)\n",
    "        print(\"SAM data config:\", data_config)\n",
    "        self.preprocess = timm.data.create_transform(**data_config, is_training=False)\n",
    "        \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract normalized feature embeddings from a given image.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError('Model not loaded')\n",
    "\n",
    "        image_tensor = [self.preprocess(img).to(self.device) for img in image]\n",
    "        image_tensor = torch.stack(image_tensor).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model(image_tensor)\n",
    "            \n",
    "        return self.normalize_embedding(features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_embedding(embs):\n",
    "        \"\"\"\n",
    "        Normalize the embedding vectors to have unit length.\n",
    "        \"\"\"\n",
    "        return torch.nn.functional.normalize(embs.float(), dim=1, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "formatted"
    ]
   },
   "outputs": [],
   "source": [
    "class FungiTastic(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Dataset class for the FewShot subset of the Danish Fungi dataset (size 300, closed-set).\n",
    "\n",
    "    This dataset loader supports training, validation, and testing splits, and provides\n",
    "    convenient access to images, class IDs, and file paths. It also supports optional\n",
    "    image transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    SPLIT2STR = {'train': 'Train', 'val': 'Val', 'test': 'Test'}\n",
    "\n",
    "    def __init__(self, root: str, split: str = 'val', transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the FungiTastic dataset.\n",
    "\n",
    "        Args:\n",
    "            root (str): The root directory of the dataset.\n",
    "            split (str, optional): The dataset split to use. Must be one of {'train', 'val', 'test'}.\n",
    "                Defaults to 'val'.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.df = self._get_df(root, split)\n",
    "\n",
    "        assert \"image_path\" in self.df\n",
    "        if self.split != 'test':\n",
    "            assert \"category_id\" in self.df\n",
    "            self.n_classes = len(self.df['category_id'].unique())\n",
    "            self.category_id2label = {\n",
    "                k: v[0] for k, v in self.df.groupby('category_id')['species'].unique().to_dict().items()\n",
    "            }\n",
    "            self.label2category_id = {\n",
    "                v: k for k, v in self.category_id2label.items()\n",
    "            }\n",
    "\n",
    "    def add_embeddings(self, embeddings: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Updates the dataset instance with new embeddings.\n",
    "    \n",
    "        Args:\n",
    "            embeddings (pd.DataFrame): A DataFrame containing 'filename', 'transformation', \n",
    "                                      and 'embedding' columns.\n",
    "        \"\"\"\n",
    "        assert isinstance(embeddings, pd.DataFrame), \"Embeddings must be a pandas DataFrame.\"\n",
    "        assert \"embedding\" in embeddings.columns, \"Embeddings DataFrame must have an 'embedding' column.\"\n",
    "        assert \"transformation\" in embeddings.columns, \"Embeddings DataFrame must have a 'transformation' column.\"\n",
    "        \n",
    "        # Merge on both filename and transformation\n",
    "        self.df = pd.merge(self.df, embeddings, on=[\"filename\"], how=\"left\")\n",
    "        \n",
    "        # Make sure we have embeddings for at least the original images\n",
    "        assert not self.df[self.df[\"transformation\"] == \"original\"][\"embedding\"].isna().any(), \\\n",
    "            \"Missing embeddings for some original images\"\n",
    "\n",
    "    def get_embeddings_for_class(self, id):\n",
    "        # return the embeddings for class class_idx\n",
    "        class_idxs = self.df[self.df['category_id'] == id].index\n",
    "        return self.df.iloc[class_idxs]['embedding']\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_df(data_path: str, split: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the dataset metadata as a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): The root directory where the dataset is stored.\n",
    "            split (str): The dataset split to load. Must be one of {'train', 'val', 'test'}.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing metadata and file paths for the split.\n",
    "        \"\"\"\n",
    "        df_path = os.path.join(\n",
    "            data_path,\n",
    "            \"metadata\",\n",
    "            \"FungiTastic-FewShot\",\n",
    "            f\"FungiTastic-FewShot-{FungiTastic.SPLIT2STR[split]}.csv\"\n",
    "        )\n",
    "        df = pd.read_csv(df_path)\n",
    "        df[\"image_path\"] = df.filename.apply(\n",
    "            lambda x: os.path.join(data_path, \"FungiTastic-FewShot\", split, 'fullsize', x)  # TODO: 300p to fullsize if different embedder that can handle it\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Retrieves a single data sample by index.\n",
    "    \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "            ret_image (bool, optional): Whether to explicitly return the image. Defaults to False.\n",
    "    \n",
    "        Returns:\n",
    "            tuple:\n",
    "                - If embeddings exist: (image?, embedding, category_id, file_path)\n",
    "                - If no embeddings: (image, category_id, file_path) (original version)\n",
    "        \"\"\"\n",
    "        file_path = self.df[\"image_path\"].iloc[idx].replace('FungiTastic-FewShot', 'images/FungiTastic-FewShot')\n",
    "    \n",
    "        if self.split != 'test':\n",
    "            category_id = self.df[\"category_id\"].iloc[idx]\n",
    "        else:\n",
    "            category_id = None\n",
    "\n",
    "        image = Image.open(file_path)\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        # Check if embeddings exist\n",
    "        if \"embedding\" in self.df.columns:\n",
    "            emb = torch.tensor(self.df.iloc[idx]['embedding'], dtype=torch.float32).squeeze()\n",
    "        else:\n",
    "            emb = None  # No embeddings available\n",
    "    \n",
    "\n",
    "        return image, category_id, file_path, emb\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_class_id(self, idx: int) -> int:\n",
    "        \"\"\"\n",
    "        Returns the class ID of a specific sample.\n",
    "        \"\"\"\n",
    "        return self.df[\"category_id\"].iloc[idx]\n",
    "\n",
    "    def show_sample(self, idx: int) -> None:\n",
    "        \"\"\"\n",
    "        Displays a sample image along with its class name and index.\n",
    "        \"\"\"\n",
    "        image, category_id, _, _ = self.__getitem__(idx)\n",
    "        class_name = self.category_id2label[category_id]\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Class: {class_name}; id: {idx}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def get_category_idxs(self, category_id: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves all indexes for a given category ID.\n",
    "        \"\"\"\n",
    "        return self.df[self.df.category_id == category_id].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the datasets\n",
    "\n",
    "train_dataset = FungiTastic(root=data_path, split='train', transform=None)\n",
    "val_dataset = FungiTastic(root=data_path, split='val', transform=None)\n",
    "test_dataset = FungiTastic(root=data_path, split='test', transform=None)\n",
    "\n",
    "train_dataset.df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize few samples\n",
    "\n",
    "train_dataset.show_sample(1), train_dataset.show_sample(500), train_dataset.show_sample(1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_dir = data_path / \"images\" / \"FungiTastic-FewShot\" / \"test\" / \"fullsize\"\n",
    "val_image_dir = data_path / \"images\" / \"FungiTastic-FewShot\" / \"val\" / \"fullsize\"\n",
    "train_image_dir = data_path / \"images\" / \"FungiTastic-FewShot\" / \"train\" / \"fullsize\"\n",
    "image_extensions = {'.JPG'}\n",
    "\n",
    "def show_image_details_for_dir(image_dir):\n",
    "    image_sizes = []\n",
    "    for filename in image_dir.iterdir():\n",
    "        if filename.suffix in image_extensions:\n",
    "            path = os.path.join(image_dir, filename)\n",
    "            try:\n",
    "                with Image.open(path) as img:\n",
    "                    w, h = img.size\n",
    "                    image_sizes.append((filename, w, h))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to open {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Checked {len(image_sizes)} images.\")\n",
    "    for fname, w, h in image_sizes[:10]:  # show a few\n",
    "        print(f\"{fname}: {w}x{h}\")\n",
    "    \n",
    "    widths = [w for _, w, _ in image_sizes]\n",
    "    heights = [h for _, _, h in image_sizes]\n",
    "    \n",
    "    print(f\"Min size: {min(widths)}x{min(heights)}\")\n",
    "    print(f\"Max size: {max(widths)}x{max(heights)}\")\n",
    "    print(f\"Avg size: {sum(widths)//len(widths)}x{sum(heights)//len(heights)}\")\n",
    "    return image_sizes\n",
    "\n",
    "train_images = show_image_details_for_dir(train_image_dir)\n",
    "val_images = show_image_details_for_dir(val_image_dir)\n",
    "test_images = show_image_details_for_dir(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = {\n",
    "                 # \"DINOv2Base@434\": DINOv2B, \n",
    "                 # \"FungiTasticBEIT@384\": FungiTasticBEIT,\n",
    "                 # \"DINOv2Large@518\": DINOv2L,\n",
    "                 \"SAMViTH@1024\": SAMViTH,\n",
    "                 }\n",
    "\n",
    "models = []\n",
    "for model_cls in model_classes.values():\n",
    "    curr_model = model_cls(device=device)\n",
    "    curr_model.load()\n",
    "    curr_model.eval()\n",
    "    models.append(curr_model)\n",
    "\n",
    "def generate_embeddings(dataset):\n",
    "    idxs = np.arange(len(dataset))\n",
    "    data = []\n",
    "    \n",
    "    def original(img):\n",
    "        return img\n",
    "    \n",
    "    def horizontal_flip(img):\n",
    "        if hasattr(img, 'transpose'):  # PIL Image\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        else:  # Tensor\n",
    "            return torch.flip(img, dims=[2])\n",
    "\n",
    "    def aspect_preserving_center_crop(pil_img, scale=0.8):\n",
    "        \"\"\"\n",
    "        Returns an aspect-preserving center crop of the input PIL image.\n",
    "        The crop is `scale` times the original size (e.g., 0.8 for 80% of original).\n",
    "        \"\"\"\n",
    "        w, h = pil_img.size\n",
    "        crop_w = int(scale * w)\n",
    "        crop_h = int(scale * h)\n",
    "    \n",
    "        left = (w - crop_w) // 2\n",
    "        top = (h - crop_h) // 2\n",
    "        right = left + crop_w\n",
    "        bottom = top + crop_h\n",
    "    \n",
    "        return pil_img.crop((left, top, right, bottom))\n",
    "\n",
    "    \n",
    "    def tlcrop_no_resize(pil_img, scale=0.8):\n",
    "        \"\"\"\n",
    "        Return four aspect-preserving corner crops, each of size\n",
    "        (scale * original height, scale * original width).\n",
    "        \"\"\"\n",
    "        w, h = pil_img.size\n",
    "        crop_w = int(scale * w)\n",
    "        crop_h = int(scale * h)\n",
    "        box = (0, 0, crop_w, crop_h)  # Top-left\n",
    "        return pil_img.crop(box)\n",
    "\n",
    "    \n",
    "    def trcrop_no_resize(pil_img, scale=0.8):\n",
    "        \"\"\"\n",
    "        Return four aspect-preserving corner crops, each of size\n",
    "        (scale * original height, scale * original width).\n",
    "        \"\"\"\n",
    "        w, h = pil_img.size\n",
    "        crop_w = int(scale * w)\n",
    "        crop_h = int(scale * h)\n",
    "        box = (w - crop_w, 0, w, crop_h)  # Top-right\n",
    "        return pil_img.crop(box)\n",
    "\n",
    "    \n",
    "    def blcrop_no_resize(pil_img, scale=0.8):\n",
    "        \"\"\"\n",
    "        Return four aspect-preserving corner crops, each of size\n",
    "        (scale * original height, scale * original width).\n",
    "        \"\"\"\n",
    "        w, h = pil_img.size\n",
    "        crop_w = int(scale * w)\n",
    "        crop_h = int(scale * h)\n",
    "        box = (0, h - crop_h, crop_w, h)  # Bottom-left\n",
    "        return pil_img.crop(box)\n",
    "\n",
    "    \n",
    "    def brcrop_no_resize(pil_img, scale=0.8):\n",
    "        \"\"\"\n",
    "        Return four aspect-preserving corner crops, each of size\n",
    "        (scale * original height, scale * original width).\n",
    "        \"\"\"\n",
    "        w, h = pil_img.size\n",
    "        crop_w = int(scale * w)\n",
    "        crop_h = int(scale * h)\n",
    "        box = (w - crop_w, h - crop_h, w, h)  # Bottom-right\n",
    "        return pil_img.crop(box)\n",
    "\n",
    "    def rot_90(img):\n",
    "        return F.rotate(img, 90)\n",
    "        \n",
    "    def rot_180(img):\n",
    "        return F.rotate(img, 180)\n",
    "        \n",
    "    def rot_270(img):\n",
    "        return F.rotate(img, 270)\n",
    "        \n",
    "    def rot_15(img):\n",
    "        return img.rotate(15, resample=Image.BICUBIC, expand=False)\n",
    "        \n",
    "    def rot_345(img):\n",
    "        return img.rotate(345, resample=Image.BICUBIC, expand=False)\n",
    "    \n",
    "    transforms = {\n",
    "        \"original\": original,\n",
    "        \"horizontal_flip\": horizontal_flip,\n",
    "        \"rot_90\": rot_90,\n",
    "        \"rot_270\": rot_270,\n",
    "        \"rot_15\": rot_15,\n",
    "        \"rot_345\": rot_345,\n",
    "    }\n",
    "\n",
    "    crop_transforms = {\n",
    "        \"center_crop\": aspect_preserving_center_crop,\n",
    "        \"top_left_crop\": tlcrop_no_resize,\n",
    "        \"top_right_crop\": trcrop_no_resize,\n",
    "        \"bottom_left_crop\": blcrop_no_resize,\n",
    "        \"bottom_right_crop\": brcrop_no_resize,\n",
    "    }\n",
    "    \n",
    "    logged_dims = False\n",
    "    for idx in tqdm(idxs):\n",
    "        im, label, file_path, _ = dataset[idx]\n",
    "        original_filename = dataset.df[\"filename\"].iloc[idx]\n",
    "\n",
    "        for transforms_dict in [transforms, crop_transforms]:  # two distinct sizes so can't be batched together\n",
    "            transformed_images = []\n",
    "            transform_names = []\n",
    "            for transform_name, transform in transforms_dict.items():\n",
    "                transformed_im = transform(im)\n",
    "                if transformed_im.mode != 'RGB':\n",
    "                    transformed_im = transformed_im.convert('RGB')\n",
    "                transformed_images.append(transformed_im)\n",
    "                transform_names.append(transform_name)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if len(models) > 1:\n",
    "                    feats = []\n",
    "                    emb_dims = []\n",
    "                    for model in models:\n",
    "                        feat = model.extract_features(transformed_images)\n",
    "                        feat = feat.detach().cpu().numpy()\n",
    "                        emb_dims.append(feat.shape[-1])\n",
    "                        feats.append(feat)\n",
    "                    feats = np.concatenate(feats, axis=1)\n",
    "                \n",
    "                if not logged_dims:\n",
    "                    print(\"emb_dims:\", emb_dims)\n",
    "                    logged_dims = True\n",
    "            \n",
    "            for feats_, transform_name in zip(feats, transform_names):\n",
    "                data.append({\n",
    "                    'filename': original_filename,\n",
    "                    'transformation': transform_name,\n",
    "                    'embedding': feats_,\n",
    "                    'emb_dims': emb_dims,\n",
    "                })\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    embeddings = pd.DataFrame(data)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.df.image_path.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least one image at fullsize is truncated\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(test_dataset)\n",
    "test_dataset.add_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(train_dataset)\n",
    "train_dataset.add_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(val_dataset)\n",
    "val_dataset.add_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = FungiTastic(root=data_path, split='val', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"SAMH_cache\"\n",
    "config = {\n",
    "    \"exp_name\": exp_name,\n",
    "    \"load_image_size\": \"fullsize\",\n",
    "    \"models\": list(model_classes.keys()),\n",
    "    \"augs\": [\"crops\", \"hflip\", \"90rotations\", \"15rotations\"],\n",
    "    \"train_time_augs\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "    \n",
    "def save_artifacts(exp_name, train_dataset, val_dataset, test_dataset, config, overwrite=False):\n",
    "    file = Path(f\"numpy_embed_dims_{exp_name}.npy\")\n",
    "    if file.exists() and not overwrite:\n",
    "        raise FileExistsError(\"overwrite is False and artifacts exist.\")\n",
    "    embed_dims = test_dataset.df.emb_dims.iloc[0]\n",
    "    np.save(f\"numpy_embed_dims_{exp_name}.npy\", embed_dims)\n",
    "    train_dataset.df.to_csv(f\"train_df_{exp_name}.csv\", index=None)\n",
    "    val_dataset.df.to_csv(f\"val_df_{exp_name}.csv\", index=None)\n",
    "    test_dataset.df.to_csv(f\"test_df_{exp_name}.csv\", index=None)\n",
    "    np.save(f\"train_numpy_embedding_{exp_name}.npy\", train_dataset.df.embedding.to_numpy())\n",
    "    np.save(f\"val_numpy_embedding_{exp_name}.npy\", val_dataset.df.embedding.to_numpy())\n",
    "    np.save(f\"test_numpy_embedding_{exp_name}.npy\", test_dataset.df.embedding.to_numpy())\n",
    "    with open(f\"config_{exp_name}.json\", \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "\n",
    "def load_artifacts(exp_name):\n",
    "    train_df = pd.read_csv(f\"train_df_{exp_name}.csv\")\n",
    "    val_df = pd.read_csv(f\"val_df_{exp_name}.csv\")\n",
    "    test_df = pd.read_csv(f\"test_df_{exp_name}.csv\")\n",
    "    embed_dims = np.load(f\"numpy_embed_dims_{exp_name}.npy\", allow_pickle=True)\n",
    "    train_df['embed_dims'] = train_df.apply(lambda row: embed_dims, axis=1)\n",
    "    val_df['embed_dims'] = val_df.apply(lambda row: embed_dims, axis=1)\n",
    "    test_df['embed_dims'] = test_df.apply(lambda row: embed_dims, axis=1)\n",
    "    train_embeddings = np.load(f\"train_numpy_embedding_{exp_name}.npy\", allow_pickle=True)\n",
    "    val_embeddings = np.load(f\"val_numpy_embedding_{exp_name}.npy\", allow_pickle=True)\n",
    "    test_embeddings = np.load(f\"test_numpy_embedding_{exp_name}.npy\", allow_pickle=True)\n",
    "    train_df[\"embedding\"] = train_embeddings\n",
    "    val_df[\"embedding\"] = val_embeddings\n",
    "    test_df[\"embedding\"] = test_embeddings\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_artifacts(exp_name, train_dataset, val_dataset, test_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = FungiTastic(root=data_path, split='train', transform=None)\n",
    "# val_dataset = FungiTastic(root=data_path, split='val', transform=None)\n",
    "# test_dataset = FungiTastic(root=data_path, split='test', transform=None)\n",
    "\n",
    "# train_dataset.df, val_dataset.df, test_dataset.df = load_artifacts(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12156235,
     "sourceId": 91448,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "fungiclef2025",
   "language": "python",
   "name": "fungiclef2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
